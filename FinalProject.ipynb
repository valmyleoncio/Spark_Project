{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto Final: Análise de Dados usando Spark\n",
    "\n",
    "O presente projeto implica na criação de um DataFrame mediante o carregamento de dados provenientes de um arquivo CSV, seguido pela aplicação de transformações e ações por meio do Spark SQL. Para atingir tal objetivo, é necessário realizar as seguintes tarefas:\n",
    "\n",
    "- Tarefa 1: Gerar DataFrame a partir dos dados CSV.\n",
    "- Tarefa 2: Definir um esquema para os dados.\n",
    "- Tarefa 3: Exibir o esquema do DataFrame.\n",
    "- Tarefa 4: Criar uma visão temporária.\n",
    "- Tarefa 5: Executar uma consulta SQL.\n",
    "- Tarefa 6: Calcular o Salário Médio por Departamento.\n",
    "- Tarefa 7: Filtrar e Exibir Funcionários do Departamento de TI.\n",
    "- Tarefa 8: Adicionar Bônus de 10% aos Salários.\n",
    "- Tarefa 9: Encontrar o Salário Máximo por Idade.\n",
    "- Tarefa 10: Auto-Junção nos Dados dos Funcionários.\n",
    "- Tarefa 11: Calcular a Idade Média do Funcionário.\n",
    "- Tarefa 12: Calcular o Salário Total por Departamento.\n",
    "- Tarefa 13: Ordenar Dados por Idade e Salário.\n",
    "- Tarefa 14: Contar Funcionários em Cada Departamento.\n",
    "- Tarefa 15: Filtrar Funcionários com a letra \"o\" no Nome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-requisitos\n",
    "\n",
    "1. Para esta tarefa de laboratório, você estará usando Python e Spark (PySpark). Portanto, é essencial garantir que as seguintes bibliotecas estejam instaladas em seu ambiente de laboratório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (3.4.2)\n",
      "Requirement already satisfied: findspark in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.0.1)\n",
      "Requirement already satisfied: wget in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Instalando pacotes necessários\n",
    "!pip install pyspark  findspark wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark é a API Spark para Python. Neste laboratório, utilizamos o PySpark para inicializar o SparkContext.\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/15 15:50:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Criando um objeto SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Criando uma SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark DataFrames basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download dos dados CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employees (2).csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Faça o download dos dados CSV primeiro para um arquivo local chamado `employees.csv`\n",
    "import wget\n",
    "wget.download(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarefas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 1: Gerar um DataFrame Spark a partir dos dados CSV\n",
    "\n",
    "Leia os dados do arquivo CSV fornecido, `employees.csv`, e importe-os para uma variável DataFrame Spark chamada `employees_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+\n",
      "|   _c0|      _c1|   _c2|_c3|       _c4|\n",
      "+------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|\n",
      "|   198|   Donald|  2600| 29|        IT|\n",
      "|   199|  Douglas|  2600| 34|     Sales|\n",
      "|   200| Jennifer|  4400| 36| Marketing|\n",
      "|   201|  Michael| 13000| 32|        IT|\n",
      "|   202|      Pat|  6000| 39|        HR|\n",
      "|   203|    Susan|  6500| 36| Marketing|\n",
      "|   204|  Hermann| 10000| 29|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|\n",
      "|   206|  William|  8300| 37|        IT|\n",
      "|   100|   Steven| 24000| 39|        IT|\n",
      "|   101|    Neena| 17000| 27|     Sales|\n",
      "|   102|      Lex| 17000| 37| Marketing|\n",
      "|   103|Alexander|  9000| 39| Marketing|\n",
      "|   104|    Bruce|  6000| 38|        IT|\n",
      "|   105|    David|  4800| 39|        IT|\n",
      "|   106|    Valli|  4800| 38|     Sales|\n",
      "|   107|    Diana|  4200| 35|     Sales|\n",
      "|   108|    Nancy| 12008| 28|     Sales|\n",
      "|   109|   Daniel|  9000| 35|        HR|\n",
      "+------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leia os dados do arquivo CSV \"emp\" e importe-os para uma variável DataFrame chamada \"employees_df\"\n",
    "employees_df = spark.read.csv('employees.csv')\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 2: Definir um esquema para os dados\n",
    "\n",
    "Construa um esquema para os dados de entrada e, em seguida, utilize o esquema definido para ler o arquivo CSV e criar um DataFrame chamado `employees_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+----------+\n",
      "| id|     name|salary|age|Department|\n",
      "+---+---------+------+---+----------+\n",
      "|198|   Donald|  2600| 29|        IT|\n",
      "|199|  Douglas|  2600| 34|     Sales|\n",
      "|200| Jennifer|  4400| 36| Marketing|\n",
      "|201|  Michael| 13000| 32|        IT|\n",
      "|202|      Pat|  6000| 39|        HR|\n",
      "|203|    Susan|  6500| 36| Marketing|\n",
      "|204|  Hermann| 10000| 29|   Finance|\n",
      "|205|  Shelley| 12008| 33|   Finance|\n",
      "|206|  William|  8300| 37|        IT|\n",
      "|100|   Steven| 24000| 39|        IT|\n",
      "|101|    Neena| 17000| 27|     Sales|\n",
      "|102|      Lex| 17000| 37| Marketing|\n",
      "|103|Alexander|  9000| 39| Marketing|\n",
      "|104|    Bruce|  6000| 38|        IT|\n",
      "|105|    David|  4800| 39|        IT|\n",
      "|106|    Valli|  4800| 38|     Sales|\n",
      "|107|    Diana|  4200| 35|     Sales|\n",
      "|108|    Nancy| 12008| 28|     Sales|\n",
      "|109|   Daniel|  9000| 35|        HR|\n",
      "|110|     John|  8200| 31| Marketing|\n",
      "+---+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defina um esquema para os dados de entrada e leia o arquivo usando o Esquema definido pelo usuário\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Carrega o arquivo CSV, aplica o esquema\n",
    "employees_df = spark.read.csv('employees.csv', schema=schema)\n",
    "\n",
    "# Elimina linhas nulas\n",
    "employees_df = employees_df.na.drop()\n",
    "\n",
    "# Exibe o DataFrame resultante\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 3: Exibir o esquema do DataFrame\n",
    "\n",
    "Exiba o esquema do DataFrame `employees_df`, mostrando todas as colunas e seus respectivos tipos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exiba todas as colunas do DataFrame, juntamente com seus respectivos tipos de dados\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 4: Criar uma visualização temporária\n",
    "\n",
    "Crie uma visualização temporária chamada `employees` para o DataFrame `employees_df`, permitindo consultas Spark SQL nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Crie uma visualização temporária chamada \"employees\" para o DataFrame\n",
    "employees_df.createTempView('employees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 5: Executar uma consulta SQL\n",
    "\n",
    "Elabore e execute uma consulta SQL para recuperar os registros da visualização `employees` onde a idade dos funcionários ultrapassa 30 anos. Em seguida, exiba o resultado da consulta SQL, mostrando os registros filtrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+---+----------+\n",
      "| id|       name|salary|age|Department|\n",
      "+---+-----------+------+---+----------+\n",
      "|199|    Douglas|  2600| 34|     Sales|\n",
      "|200|   Jennifer|  4400| 36| Marketing|\n",
      "|201|    Michael| 13000| 32|        IT|\n",
      "|202|        Pat|  6000| 39|        HR|\n",
      "|203|      Susan|  6500| 36| Marketing|\n",
      "|205|    Shelley| 12008| 33|   Finance|\n",
      "|206|    William|  8300| 37|        IT|\n",
      "|100|     Steven| 24000| 39|        IT|\n",
      "|102|        Lex| 17000| 37| Marketing|\n",
      "|103|  Alexander|  9000| 39| Marketing|\n",
      "|104|      Bruce|  6000| 38|        IT|\n",
      "|105|      David|  4800| 39|        IT|\n",
      "|106|      Valli|  4800| 38|     Sales|\n",
      "|107|      Diana|  4200| 35|     Sales|\n",
      "|109|     Daniel|  9000| 35|        HR|\n",
      "|110|       John|  8200| 31| Marketing|\n",
      "|111|     Ismael|  7700| 32|        IT|\n",
      "|112|Jose Manuel|  7800| 34|        HR|\n",
      "|113|       Luis|  6900| 34|     Sales|\n",
      "|116|     Shelli|  2900| 37|   Finance|\n",
      "+---+-----------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta SQL para recuperar apenas os registros da Visualização onde a idade ultrapassa 30\n",
    "spark.sql(\"SELECT * FROM employees WHERE age > 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 6: Calcular Salário Médio por Departamento\n",
    "\n",
    "Elabore uma consulta SQL para recuperar o salário médio dos funcionários agrupados por departamento. Exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:================================================>       (65 + 9) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|Department|AvgSalary|\n",
      "+----------+---------+\n",
      "|     Sales|  5492.92|\n",
      "|        HR|   5837.5|\n",
      "|   Finance|   5730.8|\n",
      "| Marketing|  6633.33|\n",
      "|        IT|   7400.0|\n",
      "+----------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Consulta SQL para calcular o salário médio dos funcionários agrupados por departamento\n",
    "spark.sql(\"SELECT Department, ROUND(AVG(Salary), 2) AS AvgSalary FROM employees GROUP BY Department\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 7: Filtrar e Exibir Funcionários do Departamento de TI\n",
    "\n",
    "Aplique um filtro no DataFrame `employees_df` para selecionar registros onde o departamento é 'TI'. Exiba o DataFrame filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---+----------+\n",
      "| id|   name|salary|age|Department|\n",
      "+---+-------+------+---+----------+\n",
      "|198| Donald|  2600| 29|        IT|\n",
      "|201|Michael| 13000| 32|        IT|\n",
      "|206|William|  8300| 37|        IT|\n",
      "|100| Steven| 24000| 39|        IT|\n",
      "|104|  Bruce|  6000| 38|        IT|\n",
      "|105|  David|  4800| 39|        IT|\n",
      "|111| Ismael|  7700| 32|        IT|\n",
      "|129|  Laura|  3300| 38|        IT|\n",
      "|132|     TJ|  2100| 34|        IT|\n",
      "|136|  Hazel|  2200| 29|        IT|\n",
      "+---+-------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplique um filtro para selecionar registros onde o departamento é 'TI'\n",
    "employees_df.filter(employees_df.Department == 'IT').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 8: Adicionar Bônus de 10% aos Salários\n",
    "\n",
    "Realize uma transformação para adicionar uma nova coluna chamada \"SalaryAfterBonus\" ao DataFrame. Calcule o novo salário adicionando um bônus de 10% ao salário de cada funcionário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+----------+----------------+\n",
      "| id|     name|salary|age|Department|SalaryAfterBonus|\n",
      "+---+---------+------+---+----------+----------------+\n",
      "|198|   Donald|  2600| 29|        IT|          2860.0|\n",
      "|199|  Douglas|  2600| 34|     Sales|          2860.0|\n",
      "|200| Jennifer|  4400| 36| Marketing|          4840.0|\n",
      "|201|  Michael| 13000| 32|        IT|         14300.0|\n",
      "|202|      Pat|  6000| 39|        HR|          6600.0|\n",
      "|203|    Susan|  6500| 36| Marketing|          7150.0|\n",
      "|204|  Hermann| 10000| 29|   Finance|         11000.0|\n",
      "|205|  Shelley| 12008| 33|   Finance|         13208.8|\n",
      "|206|  William|  8300| 37|        IT|          9130.0|\n",
      "|100|   Steven| 24000| 39|        IT|         26400.0|\n",
      "|101|    Neena| 17000| 27|     Sales|         18700.0|\n",
      "|102|      Lex| 17000| 37| Marketing|         18700.0|\n",
      "|103|Alexander|  9000| 39| Marketing|          9900.0|\n",
      "|104|    Bruce|  6000| 38|        IT|          6600.0|\n",
      "|105|    David|  4800| 39|        IT|          5280.0|\n",
      "|106|    Valli|  4800| 38|     Sales|          5280.0|\n",
      "|107|    Diana|  4200| 35|     Sales|          4620.0|\n",
      "|108|    Nancy| 12008| 28|     Sales|         13208.8|\n",
      "|109|   Daniel|  9000| 35|        HR|          9900.0|\n",
      "|110|     John|  8200| 31| Marketing|          9020.0|\n",
      "+---+---------+------+---+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "#Adicione uma nova coluna \"SalaryAfterBonus\" com um bônus de 10% adicionado ao salário original, arredondando\n",
    "employees_df.withColumn(\"SalaryAfterBonus\", round(col(\"Salary\") * 1.1, 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 9: Encontrar Salário Máximo por Idade\n",
    "\n",
    "Agrupe os dados por idade e calcule o salário máximo para cada grupo de idade. Exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=====================================================>  (71 + 4) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|max(Salary)|\n",
      "+---+-----------+\n",
      "| 31|       8200|\n",
      "| 34|       7800|\n",
      "| 28|      12008|\n",
      "| 27|      17000|\n",
      "| 26|       3600|\n",
      "| 37|      17000|\n",
      "| 35|       9000|\n",
      "| 39|      24000|\n",
      "| 38|       6000|\n",
      "| 29|      10000|\n",
      "| 32|      13000|\n",
      "| 33|      12008|\n",
      "| 30|       8000|\n",
      "| 36|       7900|\n",
      "+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "# Agrupe os dados por idade e calcule o salário máximo para cada grupo de idade\n",
    "spark.sql(\"SELECT age, max(Salary) FROM employees GROUP BY age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 10: Auto-Junção nos Dados dos Funcionários\n",
    "\n",
    "Faça uma junção do DataFrame \"employees_df\" consigo mesmo com base na coluna \"id\". Exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+----------+---------+------+---+----------+\n",
      "| id|     name|salary|age|Department|     name|salary|age|Department|\n",
      "+---+---------+------+---+----------+---------+------+---+----------+\n",
      "|198|   Donald|  2600| 29|        IT|   Donald|  2600| 29|        IT|\n",
      "|199|  Douglas|  2600| 34|     Sales|  Douglas|  2600| 34|     Sales|\n",
      "|200| Jennifer|  4400| 36| Marketing| Jennifer|  4400| 36| Marketing|\n",
      "|201|  Michael| 13000| 32|        IT|  Michael| 13000| 32|        IT|\n",
      "|202|      Pat|  6000| 39|        HR|      Pat|  6000| 39|        HR|\n",
      "|203|    Susan|  6500| 36| Marketing|    Susan|  6500| 36| Marketing|\n",
      "|204|  Hermann| 10000| 29|   Finance|  Hermann| 10000| 29|   Finance|\n",
      "|205|  Shelley| 12008| 33|   Finance|  Shelley| 12008| 33|   Finance|\n",
      "|206|  William|  8300| 37|        IT|  William|  8300| 37|        IT|\n",
      "|100|   Steven| 24000| 39|        IT|   Steven| 24000| 39|        IT|\n",
      "|101|    Neena| 17000| 27|     Sales|    Neena| 17000| 27|     Sales|\n",
      "|102|      Lex| 17000| 37| Marketing|      Lex| 17000| 37| Marketing|\n",
      "|103|Alexander|  9000| 39| Marketing|Alexander|  9000| 39| Marketing|\n",
      "|104|    Bruce|  6000| 38|        IT|    Bruce|  6000| 38|        IT|\n",
      "|105|    David|  4800| 39|        IT|    David|  4800| 39|        IT|\n",
      "|106|    Valli|  4800| 38|     Sales|    Valli|  4800| 38|     Sales|\n",
      "|107|    Diana|  4200| 35|     Sales|    Diana|  4200| 35|     Sales|\n",
      "|108|    Nancy| 12008| 28|     Sales|    Nancy| 12008| 28|     Sales|\n",
      "|109|   Daniel|  9000| 35|        HR|   Daniel|  9000| 35|        HR|\n",
      "|110|     John|  8200| 31| Marketing|     John|  8200| 31| Marketing|\n",
      "+---+---------+------+---+----------+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Faça uma junção do DataFrame consigo mesmo com base na coluna \"id\"\n",
    "employees_df.join(employees_df, on=\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 11: Calcular a Idade Média dos Funcionários\n",
    "\n",
    "Calcule a idade média dos funcionários usando a função de agregação incorporada. Exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|   33.56|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcular a idade média dos funcionários\n",
    "from pyspark.sql.functions import avg \n",
    "employees_df.agg(avg(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 12: Calcular Salário Total por Departamento\n",
    "\n",
    "Calcule o salário total para cada departamento utilizando a função de agregação incorporada. Exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================================>         (62 + 11) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+-----------+\n",
      "|     Sales|      71408|\n",
      "|        HR|      46700|\n",
      "|   Finance|      57308|\n",
      "| Marketing|      59700|\n",
      "|        IT|      74000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calcule o salário total para cada departamento. Dica - Use as funções GroupBy e Aggregate\n",
    "from pyspark.sql.functions import sum \n",
    "\n",
    "# Calcular o salário total por departamento\n",
    "employees_df.groupBy(\"Department\").agg(sum(\"Salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 13: Ordenar Dados por Idade e Salário\n",
    "\n",
    "Aplique uma transformação para ordenar o DataFrame por idade em ordem ascendente e, em seguida, por salário em ordem descendente. Exiba o DataFrame ordenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---+----------+\n",
      "| id|     name|salary|age|Department|\n",
      "+---+---------+------+---+----------+\n",
      "|137|   Renske|  3600| 26| Marketing|\n",
      "|114|      Den| 11000| 27|   Finance|\n",
      "|101|    Neena| 17000| 27|     Sales|\n",
      "|108|    Nancy| 12008| 28|     Sales|\n",
      "|130|    Mozhe|  2800| 28| Marketing|\n",
      "|126|    Irene|  2700| 28|        HR|\n",
      "|140|   Joshua|  2500| 29|   Finance|\n",
      "|204|  Hermann| 10000| 29|   Finance|\n",
      "|136|    Hazel|  2200| 29|        IT|\n",
      "|198|   Donald|  2600| 29|        IT|\n",
      "|115|Alexander|  3100| 29|   Finance|\n",
      "|134|  Michael|  2900| 29|     Sales|\n",
      "|120|  Matthew|  8000| 30|        HR|\n",
      "|127|    James|  2400| 31|        HR|\n",
      "|110|     John|  8200| 31| Marketing|\n",
      "|111|   Ismael|  7700| 32|        IT|\n",
      "|119|    Karen|  2500| 32|   Finance|\n",
      "|201|  Michael| 13000| 32|        IT|\n",
      "|205|  Shelley| 12008| 33|   Finance|\n",
      "|117|    Sigal|  2800| 33|     Sales|\n",
      "+---+---------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+-----------+------+---+----------+\n",
      "| id|       name|salary|age|Department|\n",
      "+---+-----------+------+---+----------+\n",
      "|100|     Steven| 24000| 39|        IT|\n",
      "|102|        Lex| 17000| 37| Marketing|\n",
      "|101|      Neena| 17000| 27|     Sales|\n",
      "|201|    Michael| 13000| 32|        IT|\n",
      "|205|    Shelley| 12008| 33|   Finance|\n",
      "|108|      Nancy| 12008| 28|     Sales|\n",
      "|114|        Den| 11000| 27|   Finance|\n",
      "|204|    Hermann| 10000| 29|   Finance|\n",
      "|103|  Alexander|  9000| 39| Marketing|\n",
      "|109|     Daniel|  9000| 35|        HR|\n",
      "|206|    William|  8300| 37|        IT|\n",
      "|110|       John|  8200| 31| Marketing|\n",
      "|121|       Adam|  8200| 39|        HR|\n",
      "|120|    Matthew|  8000| 30|        HR|\n",
      "|122|      Payam|  7900| 36|   Finance|\n",
      "|112|Jose Manuel|  7800| 34|        HR|\n",
      "|111|     Ismael|  7700| 32|        IT|\n",
      "|113|       Luis|  6900| 34|     Sales|\n",
      "|203|      Susan|  6500| 36| Marketing|\n",
      "|123|     Shanta|  6500| 35|     Sales|\n",
      "+---+-----------+------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordene o DataFrame por idade em ordem ascendente e, em seguida, por salário em ordem descendente\n",
    "employees_df.orderBy(col(\"age\").asc()).show()\n",
    "employees_df.orderBy(col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 14: Contar Funcionários em Cada Departamento\n",
    "\n",
    "Calcule o número de funcionários em cada departamento. Exiba o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|Department|EmployeeCount|\n",
      "+----------+-------------+\n",
      "|     Sales|           13|\n",
      "|        HR|            8|\n",
      "|   Finance|           10|\n",
      "| Marketing|            9|\n",
      "|        IT|           10|\n",
      "+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Calcule o número de funcionários em cada departamento\n",
    "employees_df.groupBy(\"Department\").agg(count(\"*\").alias(\"EmployeeCount\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarefa 15: Filtrar Funcionários com a Letra 'o' no Nome\n",
    "\n",
    "Aplique um filtro para selecionar registros onde o nome do funcionário contém a letra 'o'. Exiba o DataFrame filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+---+----------+\n",
      "| id|       name|salary|age|Department|\n",
      "+---+-----------+------+---+----------+\n",
      "|198|     Donald|  2600| 29|        IT|\n",
      "|199|    Douglas|  2600| 34|     Sales|\n",
      "|110|       John|  8200| 31| Marketing|\n",
      "|112|Jose Manuel|  7800| 34|        HR|\n",
      "|130|      Mozhe|  2800| 28| Marketing|\n",
      "|133|      Jason|  3300| 38|     Sales|\n",
      "|139|       John|  2700| 36|     Sales|\n",
      "|140|     Joshua|  2500| 29|   Finance|\n",
      "+---+-----------+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplique um filtro para selecionar registros onde o nome do funcionário contém a letra 'o'\n",
    "employees_df.filter(col(\"name\").like(\"%o%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parabéns! Você concluiu o projeto.\n",
    "\n",
    "Agora você sabe como criar um DataFrame a partir de um arquivo de dados CSV e realizar uma variedade de transformações e ações de DataFrame usando o Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raghul Ramesh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lavanya T S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-09-01|0.1|Lavanya T S|Initial version|\n",
    "|2023-09-11|0.2|Pornima More|QA pass with edits|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
